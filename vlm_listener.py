#!/usr/bin/env python3
"""
VLM Listener â€” reverse-proxy that sits between the live-vlm-webui
and vLLM, intercepting every VLM response in real-time and piping it
through the Nemotron compliance checker.

Architecture
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Live WebUI â”€â”€â–º vlm_listener :8001 â”€â”€â–º vLLM (Cosmos) :8000
                      â”‚
                      â”‚  intercepts every response
                      â–¼
              compliance_checker (Nemotron via Ollama)
                      â”‚
                      â–¼
               reports/ (JSON files)

Setup
â”€â”€â”€â”€â”€
  1. Start vLLM Docker as normal (port 8000)
  2. Run:  python vlm_listener.py
  3. Point the live-vlm-webui at port 8001 instead of 8000
     (change the "API Base" field in the webui to http://<dgx-ip>:8001/v1)

That's it â€” every VLM query flows through this proxy transparently.
"""

import argparse
import asyncio
import json
import os
from datetime import datetime
from pathlib import Path

import httpx
from fastapi import FastAPI, Request, Response
import uvicorn

# â”€â”€ Local import â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from compliance_checker import check_compliance

# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
VLLM_BASE = os.getenv("VLLM_BASE_URL", "http://localhost:8000")
PROXY_PORT = int(os.getenv("PROXY_PORT", "8001"))
REPORT_DIR = Path("reports")
REPORT_DIR.mkdir(exist_ok=True)

app = FastAPI(title="VLM Compliance Proxy")

# Shared async HTTP client (created on startup)
_client: httpx.AsyncClient | None = None

# Counter for reports
_idx = 0


@app.on_event("startup")
async def _startup():
    global _client
    _client = httpx.AsyncClient(base_url=VLLM_BASE, timeout=120.0)
    print(f"ğŸ”— Proxy ready â€” forwarding to vLLM at {VLLM_BASE}")
    print(f"ğŸ“ Reports will be saved to {REPORT_DIR.resolve()}")


@app.on_event("shutdown")
async def _shutdown():
    if _client:
        await _client.aclose()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# The key endpoint: intercept /v1/chat/completions
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.post("/v1/chat/completions")
async def proxy_chat_completions(request: Request):
    global _idx
    body = await request.body()
    ts = datetime.now().strftime("%H:%M:%S")

    print(f"[{ts}] ğŸ“¨ Incoming request â†’ forwarding to vLLMâ€¦")

    # Forward the request exactly as-is to vLLM
    try:
        resp = await _client.post(
            "/v1/chat/completions",
            content=body,
            headers={"Content-Type": "application/json"},
        )
    except Exception as e:
        print(f"[{ts}] âŒ vLLM unreachable: {e}")
        return Response(
            content=json.dumps({"error": str(e)}),
            status_code=502,
            media_type="application/json",
        )

    # Parse the vLLM response
    try:
        vllm_data = resp.json()
    except Exception:
        # Not JSON â€” just pass through transparently
        print(f"[{ts}] âš ï¸  Non-JSON response from vLLM, passing through")
        return Response(content=resp.content, status_code=resp.status_code,
                        media_type=resp.headers.get("content-type", "application/json"))

    # Extract the VLM's text output
    vlm_text = ""
    try:
        vlm_text = vllm_data["choices"][0]["message"]["content"]
    except (KeyError, IndexError):
        pass

    if vlm_text:
        print(f"[{ts}] ğŸ” VLM says: {vlm_text[:150]}â€¦")

        # Try to parse VLM output as JSON
        observation = _parse_vlm_output(vlm_text)

        # Run compliance in background so we don't slow down the webui
        asyncio.create_task(_run_compliance_async(observation, _idx, ts))
        _idx += 1

    # Return the original vLLM response to the webui unchanged
    return Response(
        content=resp.content,
        status_code=resp.status_code,
        media_type=resp.headers.get("content-type", "application/json"),
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Catch-all: proxy everything else (model list, health, etc.)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.api_route("/{path:path}", methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "HEAD", "PATCH"])
async def proxy_passthrough(request: Request, path: str):
    url = f"/{path}"
    body = await request.body()

    try:
        resp = await _client.request(
            method=request.method,
            url=url,
            content=body,
            headers={k: v for k, v in request.headers.items()
                     if k.lower() not in ("host", "content-length")},
        )
        return Response(
            content=resp.content,
            status_code=resp.status_code,
            media_type=resp.headers.get("content-type"),
        )
    except Exception as e:
        return Response(
            content=json.dumps({"error": str(e)}),
            status_code=502,
            media_type="application/json",
        )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Helpers
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _parse_vlm_output(text: str) -> dict:
    """Try to parse VLM text as JSON; wrap in a dict if it fails."""
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        cleaned = text.strip().removeprefix("```json").removeprefix("```").removesuffix("```").strip()
        try:
            return json.loads(cleaned)
        except json.JSONDecodeError:
            return {
                "raw_description": text,
                "timestamp": datetime.now().isoformat(),
            }


async def _run_compliance_async(observation: dict, idx: int, ts: str):
    """Run the compliance check in a thread pool (it's sync/blocking)."""
    try:
        print(f"[{ts}] âš–ï¸  Running compliance check (background)â€¦")
        # Run blocking Ollama call in a thread so we don't block the event loop
        report = await asyncio.to_thread(check_compliance, observation, None)

        status = report.get("overall_status", "unknown")
        violations = report.get("violations", [])
        risk = report.get("risk_score", "?")

        icon = "ğŸš¨" if violations else "âœ…"
        print(f"[{ts}] {icon} Compliance: {status} | Risk: {risk}/100 | Violations: {len(violations)}")

        if violations:
            for v in violations:
                print(f"       â›” {v.get('rule', '?')}: {v.get('description', '')[:100]}")

        # Save report
        save_path = REPORT_DIR / f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{idx:04d}.json"
        combined = {
            "observation": observation,
            "compliance_report": report,
        }
        save_path.write_text(json.dumps(combined, indent=2))
        print(f"[{ts}] ğŸ’¾ Saved â†’ {save_path}")

    except Exception as e:
        print(f"[{ts}] âŒ Compliance check failed: {e}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    # FIX: Declare global immediately at the start of the function
    global VLLM_BASE 

    parser = argparse.ArgumentParser(
        description="VLM Compliance Proxy â€” intercepts vLLM responses and runs Nemotron compliance checks"
    )
    parser.add_argument("--port", type=int, default=PROXY_PORT,
                        help=f"Proxy listen port (default: {PROXY_PORT})")
    
    # Now this usage of VLLM_BASE is legal because we declared it global above
    parser.add_argument("--vllm-url", type=str, default=VLLM_BASE,
                        help=f"vLLM backend URL (default: {VLLM_BASE})")
    parser.add_argument("--host", type=str, default="0.0.0.0",
                        help="Bind address (default: 0.0.0.0)")
    args = parser.parse_args()

    # Update the global config with the argument
    VLLM_BASE = args.vllm_url

    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘       VLM Compliance Proxy                       â•‘")
    print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
    print(f"â•‘  Proxy:   http://{args.host}:{args.port}               â•‘")
    print(f"â•‘  vLLM:    {VLLM_BASE:<38} â•‘")
    print(f"â•‘  Reports: {str(REPORT_DIR.resolve()):<38} â•‘")
    print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
    print("â•‘  Point your live-vlm-webui at this proxy port!   â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()

    uvicorn.run(app, host=args.host, port=args.port, log_level="warning")


if __name__ == "__main__":
    main()